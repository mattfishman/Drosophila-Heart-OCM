{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matthew Fishman (6/24/2022)\n",
    "\n",
    "# Tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model, save_model\n",
    "from tensorflow.keras.layers import ConvLSTM2D, TimeDistributed, Conv2D, Conv3D, BatchNormalization, Conv3DTranspose\n",
    "from tensorflow.keras.layers import Activation, MaxPooling2D, MaxPool3D, Dropout, Flatten, Dense, Input, LeakyReLU, Bidirectional\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import loss_funcs as lf\n",
    "\n",
    "\n",
    "# Import numpy and data management\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from random import randint\n",
    "from random import sample\n",
    "import skimage.io\n",
    "from skimage import transform\n",
    "import cv2\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "type = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pickle file and return the ids for the samples in the training and validation sets\n",
    "def read_pk(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    data=np.array(data)\n",
    "    y=np.zeros(len(data))\n",
    "    available_ids = np.array(range(len(data)))\n",
    "    shuffle(available_ids)\n",
    "\n",
    "\n",
    "    if(len(available_ids)>=70):\n",
    "        multiplier = 0.95\n",
    "    else:\n",
    "        multiplier = 0.9\n",
    "    final_train_id = int(len(available_ids)*multiplier)\n",
    "    train_ids = available_ids[:final_train_id]\n",
    "    val_ids = available_ids[final_train_id:]\n",
    "\n",
    "    return data, train_ids, val_ids\n",
    "\n",
    "\n",
    "def getImg(path):\n",
    "    global type\n",
    "    # Get a list of files in the directory\n",
    "    files = os.listdir(path)\n",
    "    # Get the file with \"resize\" in the name\n",
    "    search_string = type + \"_img.tiff\"\n",
    "\n",
    "    for file in files:\n",
    "        if search_string in file:\n",
    "            file_path = os.path.join(path, file).replace(\"\\\\\",\"/\")\n",
    "            return file_path\n",
    "    print(\"No resize file found\")\n",
    "\n",
    "\n",
    "def getMask(path):\n",
    "    global type\n",
    "    # Get a list of files in the directory\n",
    "    files = os.listdir(path)\n",
    "    # Get the file with \"mask\" in the name\n",
    "    search_string = type + \"_mask.tiff\"\n",
    "    for file in files:\n",
    "        if search_string in file:\n",
    "            file_path = os.path.join(path, file).replace(\"\\\\\",\"/\")\n",
    "            return file_path\n",
    "    print(\"No mask file found\")\n",
    "\n",
    "def getResizeImg(path):\n",
    "    # Get a list of files in the directory\n",
    "    files = os.listdir(path)\n",
    "    # Get the file with \"resize\" in the name\n",
    "    search_string = \"resize_img.tiff\"\n",
    "\n",
    "    for file in files:\n",
    "        if search_string in file:\n",
    "            file_path = os.path.join(path, file).replace(\"\\\\\",\"/\")\n",
    "            return file_path\n",
    "    print(\"No resize file found\")\n",
    "\n",
    "def getResizeMask(path):\n",
    "    # Get a list of files in the directory\n",
    "    files = os.listdir(path)\n",
    "    # Get the file with \"mask\" in the name\n",
    "    search_string = \"resize_mask.tiff\"\n",
    "    for file in files:\n",
    "        if search_string in file:\n",
    "            file_path = os.path.join(path, file).replace(\"\\\\\",\"/\")\n",
    "            return file_path\n",
    "    print(\"No mask file found\")\n",
    "\n",
    "def centerVideo(video):\n",
    "    # Center the video by subtracting the mean of all the frames and dividing by the standard deviation\n",
    "    mean = np.mean(video)\n",
    "    std = np.std(video)\n",
    "    centered_video = (video - mean) / std\n",
    "    return centered_video\n",
    "\n",
    "def shift(arr, num, axis):\n",
    "    # Shift the array by num along the axis and fill with zeros\n",
    "    result = np.zeros_like(arr)\n",
    "    if axis == 0:\n",
    "        result[num:, :] = arr[:-num, :]\n",
    "    elif axis == 1:\n",
    "        result[:, num:] = arr[:, :-num]\n",
    "    elif axis == 2:\n",
    "        result[:, :, num:] = arr[:, :, :-num]\n",
    "    else:\n",
    "        raise ValueError(\"axis should be 0, 1 or 2\")\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(data, available_ids, batch_size):\n",
    "    #generate train data\n",
    "    augment = True\n",
    "    while True:\n",
    "        # Choose two random IDs from the available IDs\n",
    "        s = sample(list(available_ids), 3)\n",
    "        outputX = []\n",
    "        outputY = []\n",
    "        for i in s:\n",
    "            # print(i)\n",
    "            # Read the image at that ID and convert it to a numpy array\n",
    "            dir_path = data[i]\n",
    "            # print(dir_path)\n",
    "            mask_path = getMask(dir_path)\n",
    "            resize_path = getImg(dir_path)\n",
    "            img = skimage.io.imread(resize_path)\n",
    "            img = np.array(img)\n",
    "            img = np.squeeze(img)\n",
    "           \n",
    "            # Read the mask file and convert it to a numpy array\n",
    "            img_mask=skimage.io.imread(mask_path)\n",
    "            img_mask=np.array(img_mask)\n",
    "            if(len(img_mask.shape) > 3):\n",
    "                img_mask = (img_mask[:,:,:,0]>0.5)*1.0\n",
    "            else:\n",
    "                img_mask = (img_mask>0.5)*1.0\n",
    "            \n",
    "            # Add a singleton dimension so that all images have a color channel\n",
    "            train = np.array(img)\n",
    "            train=train[...,np.newaxis]\n",
    "            y=np.array(img_mask)\n",
    "            y=y[...,np.newaxis]\n",
    "            \n",
    "            # For training samples shorten to 128 frames per step\n",
    "            last_start = train.shape[0] - batch_size\n",
    "            start_loc = randint(0, last_start)\n",
    "            end_loc = start_loc + batch_size\n",
    "            train = train[start_loc:end_loc]\n",
    "            y = y[start_loc:end_loc]\n",
    "\n",
    "            # The shape of y is [frame, height, width, 1]\n",
    "            # y is a mask with value 0 or 1, find the max and min x and y coordinates of the mask\n",
    "            max_x = np.max(np.where(y == 1)[2])\n",
    "            min_x = np.min(np.where(y == 1)[2])\n",
    "            max_y = np.max(np.where(y == 1)[1])\n",
    "            min_y = np.min(np.where(y == 1)[1])\n",
    "\n",
    "            min_x = 3 if min_x <= 15 else min_x - 12\n",
    "            min_y = 3 if min_y <= 15 else min_y - 12\n",
    "            max_x = 124 if max_x >= 112 else max_x + 12\n",
    "            max_y = 596 if max_y >= 584 else max_y + 12\n",
    "            # print(min_x, max_x, min_y, max_y)\n",
    "\n",
    "            low = min_y - 30 if min_y - 30 > 1 else 1\n",
    "            high = max_y + 30 if max_y + 30 < y.shape[1] - 1 else y.shape[1] - 1\n",
    "\n",
    "            # Now select the boundaries for the crop\n",
    "            # Make sure the whole mask is in the frame\n",
    "            crop_x_min = randint(2, min_x)\n",
    "            crop_x_max = randint(max_x, 125)\n",
    "            crop_y_min = randint(low, min_y)\n",
    "            crop_y_max = randint(max_y, high)\n",
    "            # print(crop_x_min, crop_x_max, crop_y_min, crop_y_max)\n",
    "\n",
    "            # Crop the image and mask\n",
    "            train_roi = train[:, crop_y_min:crop_y_max, crop_x_min:crop_x_max]\n",
    "            y_roi = y[:, crop_y_min:crop_y_max, crop_x_min:crop_x_max]\n",
    "\n",
    "            # Now interpolate the image and mask to the original size\n",
    "            train_list = []\n",
    "            y_list = []\n",
    "            for i in range(len(train_roi)):\n",
    "                train_i = cv2.resize(train_roi[i], (128, 128),interpolation=cv2.INTER_CUBIC)\n",
    "                train_list.append(train_i)\n",
    "                y_i = cv2.resize(y_roi[i], (128, 128),interpolation=cv2.INTER_CUBIC)\n",
    "                y_list.append(y_i)\n",
    "\n",
    "            train = np.array(train_list)\n",
    "            y = np.array(y_list)\n",
    "            \n",
    "            if augment:\n",
    "                # Apply random rotation/flip augmentation\n",
    "                aug = randint(0, 2) # Equal chance for each\n",
    "                if aug==0:\n",
    "                    aug_x = train\n",
    "                    aug_y = y\n",
    "                elif aug==1:\n",
    "                    aug_x = np.flip(train, 1)\n",
    "                    aug_y = np.flip(y, 1)\n",
    "                elif aug==2:\n",
    "                    aug_x = np.flip(train, 2)\n",
    "                    aug_y = np.flip(y, 2)\n",
    "                elif aug==3:\n",
    "                    aug_x = np.flip(train, 0)\n",
    "                    aug_y = np.flip(y, 0)\n",
    "                    \n",
    "\n",
    "                # Cast to uint8 before yield\n",
    "                train = aug_x.astype('float32')\n",
    "                y = aug_y.astype('float32')\n",
    "\n",
    "                # Divide the image by 255 to normalize it\n",
    "                # train /= 255.0\n",
    "                train = centerVideo(train)\n",
    "\n",
    "            else:\n",
    "                train = train.astype('float32')\n",
    "                y = y.astype('float32')\n",
    "\n",
    "                # Divide the image by 255 to normalize it\n",
    "                # train /= 255.0\n",
    "                train = centerVideo(train)\n",
    "\n",
    "            # print(train.shape)\n",
    "            outputX.append(train)\n",
    "            outputY.append(y)\n",
    "\n",
    "        outputX = np.array(outputX)\n",
    "        outputY = np.array(outputY)\n",
    "\n",
    "        # print(outputX.shape)\n",
    "        # print(outputY.shape)\n",
    "        # print(outputX[0][0])\n",
    "        # print(outputX.shape)\n",
    "        # return outputX, outputY\n",
    "        yield (outputX, outputY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFiles(data, ids):\n",
    "    # make an empty array to hold \n",
    "    train_list = []\n",
    "    mask_list = []\n",
    "    for i in ids:\n",
    "        dir_path = data[i]\n",
    "        mask_path = getResizeMask(dir_path)\n",
    "        resize_path = getResizeImg(dir_path)\n",
    "        img = skimage.io.imread(resize_path)\n",
    "        img = np.array(img)\n",
    "        img = np.squeeze(img)\n",
    "        \n",
    "        # Read the mask file and convert it to a numpy array\n",
    "        # mask_file=input_dir+'/'+input_name+'_mask.tiff'\n",
    "        img_mask=skimage.io.imread(mask_path)\n",
    "        img_mask=np.array(img_mask)\n",
    "        if(len(img_mask.shape) > 3):\n",
    "                img_mask = (img_mask[:,:,:,0]>0.5)*1.0\n",
    "        else:\n",
    "            img_mask = (img_mask>0.5)*1.0\n",
    "        \n",
    "        # Add a singleton dimension so that all images have a color channel\n",
    "        train = np.array(img)\n",
    "        train=train[:4000,:,:,np.newaxis]\n",
    "        y=np.array(img_mask)\n",
    "        y=y[:4000,:,:,np.newaxis]\n",
    "\n",
    "        startidx = 0\n",
    "        endidx = 32\n",
    "        for i in range(4000//64):\n",
    "            # train_list.append(train[startidx:endidx, ...].astype('float32') / 255.0)\n",
    "            train_list.append(centerVideo(train[startidx:endidx, ...].astype('float32')))\n",
    "            mask_list.append(y[startidx:endidx, ...].astype('float32'))\n",
    "            startidx += 64\n",
    "            endidx += 64\n",
    "\n",
    "    # Convert the lists into numpy arrays combining the first dimension\n",
    "    train = np.array(train_list)\n",
    "    y = np.array(mask_list)\n",
    "    \n",
    "    return train, y\n",
    "\n",
    "# Define the different metrics for measuring the performance of the model\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    y_true_f = tf.cast(y_true_f, tf.float32)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.00001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5\n",
    "    lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = TimeDistributed(Conv2D(num_filters, 5, padding=\"same\"))(input)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(num_filters, 5, padding=\"same\"))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def deconv_block(input, num_filters):\n",
    "    x = TimeDistributed(Conv2D(num_filters, 5, padding=\"same\"))(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = TimeDistributed(Conv2D(num_filters, 5, padding=\"same\"))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def end_block(input, num_filters):\n",
    "    x = ConvLSTM2D(num_filters, 5, padding=\"same\", return_sequences=True)(input)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(num_filters, 5, padding=\"same\"))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
    "    return x, p\n",
    "\n",
    "def first_layer(input, num_filters):\n",
    "    x = end_block(input, num_filters)\n",
    "    p = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = TimeDistributed(Conv2DTranspose(num_filters, (2, 2), strides=(2,2), padding=\"same\"))(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = deconv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def last_layer(input, skip_features, num_filters):\n",
    "    x = TimeDistributed(Conv2DTranspose(num_filters, (2, 2), strides=(2,2), padding=\"same\"))(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = end_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def create_model(input_shape=(None, 128, 128, 1)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    s1, p1 = first_layer(inputs, 40)\n",
    "    s2, p2 = encoder_block(p1, 64)\n",
    "    s3, p3 = encoder_block(p2, 128)\n",
    "\n",
    "    d1 = conv_block(p3, 256)\n",
    "\n",
    "    d2 = decoder_block(d1, s3, 128)\n",
    "    d3 = decoder_block(d2, s2, 64)\n",
    "    d4 = last_layer(d3, s1, 32)\n",
    "\n",
    "    classify = Conv3D(1, (1, 1, 1), padding=\"same\", activation='sigmoid')(d4)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=classify)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main function that will train the model\n",
    "def main():\n",
    "    directory = \"C:/Python/Matthew/Training\"\n",
    "    # steps_per_id = int(400/batch_size) + 1\n",
    "\n",
    "    data, train_ids,val_ids = read_pk(\"C:/Python/Matthew/Training/full_90_804.pk\")\n",
    "    K.set_image_data_format('channels_last')\n",
    "    batch_size = 32\n",
    "    steps_per_epoch = (len(train_ids)*40)//batch_size\n",
    "    print(len(val_ids))\n",
    "    val, y_val = readFiles(data, val_ids)\n",
    "\n",
    "    loss_function = lf.Semantic_loss_functions().log_cosh_dice_loss\n",
    "\n",
    "    # model = create_model()\n",
    "    model = load_model(\"C:/Python/Matthew/Training/LSTM_sci_809_540.h5\", custom_objects = {'dice_coeff': dice_coeff, 'bce_dice_loss': bce_dice_loss, \"focal_tversky\": lf.Semantic_loss_functions().focal_tversky, \"log_cosh_dice_loss\": lf.Semantic_loss_functions().log_cosh_dice_loss})\n",
    "    # lr = 2.5e-3\n",
    "    lr = 1e-4\n",
    "    model.compile(\n",
    "        loss=loss_function,\n",
    "        optimizer=Adam(lr),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanIoU(num_classes=2),\n",
    "            tf.keras.metrics.Recall(),\n",
    "            tf.keras.metrics.Precision(),\n",
    "            dice_coeff\n",
    "        ]\n",
    "    )\n",
    "    # print(model.summary(line_length=150))\n",
    "    model_checkpoint = ModelCheckpoint(directory + \"/LSTM_sci_809_6{epoch:02d}.h5\", monitor='val_loss', save_best_only=False)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"C:/Python/Matthew/Training/LSTMLog6\", histogram_freq=1)\n",
    "    model.fit(generateData(data, train_ids, batch_size), steps_per_epoch=steps_per_epoch, epochs=40, verbose=1, validation_data=(val, y_val), callbacks=[model_checkpoint, tensorboard_callback])\n",
    "\n",
    "    return data, val_ids\n",
    "\n",
    "data, val_ids = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('TF2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5babbcaf2abf2b64ffc16d064d06e32e4554c2223239598db6b7c8ea919967b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
